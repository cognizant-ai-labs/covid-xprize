{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge tests data with oxford database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data remove spaces from column names...\n",
    "covid_tests = (pd.read_csv(\"../data_sources/tests_latest.csv\", \n",
    "                 parse_dates=['Date'],\n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 dtype={\"RegionName\": str,\n",
    "                        \"RegionCode\": str},\n",
    "                 error_bad_lines=False)\n",
    "                .rename({'ISO code': 'Code'}, axis=1))\n",
    "covid_tests.columns = covid_tests.columns.str.replace(' ', '_')\n",
    "# drop rows with null Code\n",
    "covid_tests = covid_tests[covid_tests.Code.notna()]\n",
    "covid_tests.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford = pd.read_csv(\"../data_sources/OxCGRT_latest.csv\", \n",
    "                 parse_dates=['Date'],\n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 dtype={\"RegionName\": str,\n",
    "                        \"RegionCode\": str},\n",
    "                 error_bad_lines=False)\n",
    "oxford.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to merge on two levels: country code and date, so lets index the dataframes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_tests = covid_tests.set_index(['Code', 'Date'])\n",
    "covid_tests.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford = oxford.set_index(['CountryCode', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oxford_tests =(oxford\n",
    "#                .join(covid_tests.rename_axis(oxford.index.names), how='left')\n",
    "#                .drop(['Entity', 'new_tests_per_thousand_7day_smoothed Annotations'], axis=1)\n",
    "#                .rename({'new_tests_per_thousand_7day_smoothed': 'covid_tests'})\n",
    "#               )\n",
    "oxford_tests =(oxford\n",
    "               .join(covid_tests.rename_axis(oxford.index.names), how='left')\n",
    "#                .drop(['Entity', 'new_tests_per_thousand_7day_smoothed Annotations'], axis=1)\n",
    "#                .rename({'new_tests_per_thousand_7day_smoothed': 'covid_tests'})\n",
    "              )\n",
    "oxford_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this has lots of NaN values in the tests column, but we can find a way to deal with those later when we use it on a predictor. Fpr the time being, lets save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford_tests.to_csv(\"../data_sources/OxCGRT_latest_with_tests.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pack it up in a function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_test_data(oxford_path, tests_path):\n",
    "    covid_tests = (pd.read_csv(tests_path, \n",
    "                     parse_dates=['Date'],\n",
    "                     encoding=\"ISO-8859-1\",\n",
    "                     dtype={\"RegionName\": str,\n",
    "                            \"RegionCode\": str},\n",
    "                     error_bad_lines=False)\n",
    "                    .rename({'ISO code': 'Code'}, axis=1)\n",
    "                  )\n",
    "    covid_tests.columns = covid_tests.columns.str.replace(' ', '_')\n",
    "    # drop rows with null Code\n",
    "    covid_tests = covid_tests[covid_tests[\"Code\"].notna()]\n",
    "    covid_tests = covid_tests.set_index(['Code', 'Date'])\n",
    "    oxford = pd.read_csv(oxford_path, \n",
    "                 parse_dates=['Date'],\n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 dtype={\"RegionName\": str,\n",
    "                        \"RegionCode\": str},\n",
    "                 error_bad_lines=False)\n",
    "    oxford = oxford.set_index(['CountryCode', 'Date'])\n",
    "    oxford_tests =(oxford\n",
    "                   .join(covid_tests.rename_axis(oxford.index.names), how='left')\n",
    "                  )\n",
    "    return oxford_tests.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford_tests = add_test_data(\"../data_sources/OxCGRT_latest.csv\", \"../data_sources/tests_latest.csv\")\n",
    "oxford_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Oxford and test data\n",
    "\n",
    "Lets wrap up the workflow to update and merge the Oxford and tests datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_OxCGRT_tests():\n",
    "    # source of latest Oxford data\n",
    "    OXFORD_URL = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "    # source of latest test data\n",
    "    TESTS_URL = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/testing/covid-testing-all-observations.csv\"\n",
    "    # store them locally\n",
    "    OXFORD_FILE = '../data_sources/OxCGRT_latest.csv'\n",
    "    TESTS_FILE = '../data_sources/tests_latest.csv'\n",
    "    urllib.request.urlretrieve(OXFORD_URL, OXFORD_FILE)\n",
    "    urllib.request.urlretrieve(TESTS_URL, TESTS_FILE)\n",
    "    return add_test_data(OXFORD_FILE, TESTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_oxford_tests = update_OxCGRT_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_oxford_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country characterization\n",
    "\n",
    "Now we are going to mcollect some static data aboput each country to try to capture how the different NPIs might work nacroos countries.\n",
    "\n",
    "We have the folloing datasets (gathered from [Our World in Data (OWD)](https://ourworldindata.org/)):\n",
    "\n",
    "* Economic Freedom\n",
    "* Gross per capita National Income\n",
    "* Human Development Index\n",
    "* Human Rights Scores\n",
    "* Life Expectancy\n",
    "* Mean Years of schooling\n",
    "* Political Regime \n",
    "\n",
    "All of these datasets are timeseries with measurements for different points in time. In the simplest scenario we're going to take the most recent measure for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = pd.read_csv(\"../data_sources/economic-freedom.csv\")\n",
    "# Check we have the same final year for all observations\n",
    "last_year = ef.Year.max()\n",
    "print(len(ef.Code.unique()), (ef.groupby('Code')[['Year']].max() == last_year)['Year'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every dataset from OWD has the same basic structure, so we can easily check if every dataset has all observetions for the last measured Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_final_year(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    last_year = df.Year.max()\n",
    "    try:\n",
    "        assert len(df.Code.unique()) == (df.groupby('Code')[['Year']].max() == last_year)['Year'].sum()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"../data_sources/economic-freedom.csv\",\n",
    "         \"../data_sources/gross-national-income-per-capita.csv\",\n",
    "         \"../data_sources/human-development-index.csv\",\n",
    "         \"../data_sources/life-expectancy.csv\",\n",
    "         \"../data_sources/mean-years-of-schooling-long-run.csv\",\n",
    "         \"../data_sources/political-regime-updated2016-distinction-democracies-and-full-democracies.csv\"]\n",
    "for p in paths:\n",
    "    print(p, check_all_final_year(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are some missing observations for the last year in most datasets. The simplest way to deal with those is a forward fill, propagating forward the last valid observation for each country.\n",
    "\n",
    "It is also important to notice that the datasets contain regional and global values without countr codes so we need to clean those beforehand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_sources/human-development-index.csv\")\n",
    "# remove missing country codes\n",
    "df = df[df['Code'].notna()]\n",
    "# forward fill for each country\n",
    "df['Human Development Index (UNDP)'] = (df\n",
    "                                       .groupby('Code')[['Human Development Index (UNDP)']]\n",
    "                                       .apply(lambda x: x.fillna(method='ffill'))\n",
    "                                      )\n",
    "# check that we don't have any repeated Codes\n",
    "df = df[df['Year'] == df.Year.max()]\n",
    "(df.groupby('Code').size() > 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Npw lets get a DataFrame with every country in the Oxford database and all variables from OWD. Of course we're going to get lots of NAs, we'll deal with those later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.DataFrame(oxford['CountryCode'].unique(), columns=['CountryCode'])\n",
    "rename_dict = {'Economic Freedom of the World': 'economic_freedom',\n",
    "               'GNI per capita, PPP (constant 2011 international $)': 'gni_per_capita',\n",
    "                'Human Development Index (UNDP)': 'human_development',\n",
    "                'Life expectancy': 'life_expectancy',\n",
    "                'Average Total Years of Schooling for Adult Population (Lee-Lee (2016), Barro-Lee (2018) and UNDP (2018))': 'average_years_school',\n",
    "                'Political Regime (OWID based on Polity IV and Wimmer & Min)': 'political_regime'\n",
    "}\n",
    "data_columns = {\"../data_sources/economic-freedom.csv\": 'Economic Freedom of the World',\n",
    "                 \"../data_sources/gross-national-income-per-capita.csv\": 'GNI per capita, PPP (constant 2011 international $)',\n",
    "                 \"../data_sources/human-development-index.csv\": 'Human Development Index (UNDP)',\n",
    "                 \"../data_sources/life-expectancy.csv\": 'Life expectancy',\n",
    "                 \"../data_sources/mean-years-of-schooling-long-run.csv\": 'Average Total Years of Schooling for Adult Population (Lee-Lee (2016), Barro-Lee (2018) and UNDP (2018))',\n",
    "                 \"../data_sources/political-regime-updated2016-distinction-democracies-and-full-democracies.csv\": 'Political Regime (OWID based on Polity IV and Wimmer & Min)'    \n",
    "}\n",
    "\n",
    "for p in paths:\n",
    "    df = pd.read_csv(p)\n",
    "    df = df[df['Code'].notna()]\n",
    "    df[data_columns[p]] = (df\n",
    "                                .groupby('Code')[[data_columns[p]]]\n",
    "                                .apply(lambda x: x.fillna(method='ffill'))\n",
    "                                )\n",
    "    df = df[df['Year'] == df.Year.max()]\n",
    "    countries = countries.merge(df, left_on='CountryCode', right_on='Code', how='left')\n",
    "    \n",
    "countries = (countries.rename(rename_dict, axis=1)[['CountryCode'] + list(rename_dict.values())])\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save this dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.to_csv(\"../data_sources/owd_by_country.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute OWD\n",
    "\n",
    "To impute OWD data we're going to use (a slightly modofied) the IMF economic regions and impute the average values over each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"../data_sources/owd_by_country.csv\").drop('Unnamed: 0', axis=1)\n",
    "regions = pd.read_csv(\"../data_sources/imf_economic_regions.csv\").drop('Unnamed: 0', axis=1)\n",
    "countries = countries.merge(regions[['CountryCode', 'imf_region']], on='CountryCode')\n",
    "countries = (countries\n",
    "             .groupby('imf_region')\n",
    "             .apply(lambda group: group.fillna(group.mean()))\n",
    "             .drop('imf_region', axis=1)\n",
    "             .reset_index()\n",
    "             .drop('level_1', axis=1)\n",
    "            )\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries[countries.CountryCode=='AZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.to_csv(\"../data_sources/owd_by_country_imputed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge additional data sources\n",
    "\n",
    "Now we're going to merge population data for all countries and regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from nixtamalai.helpers import preprocess_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_full()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data_sources/\"\n",
    "US_PREFIX = \"United States / \"\n",
    "ADDITIONAL_CONTEXT_FILE = os.path.join(DATA_PATH, \"Additional_Context_Data_Global.csv\")\n",
    "ADDITIONAL_US_STATES_CONTEXT = os.path.join(DATA_PATH, \"US_states_populations.csv\")\n",
    "ADDITIONAL_UK_CONTEXT = os.path.join(DATA_PATH, \"uk_populations.csv\")\n",
    "ADDITIONAL_BRAZIL_CONTEXT = os.path.join(DATA_PATH, \"brazil_populations.csv\")\n",
    "# world population by country\n",
    "country_pop_df = pd.read_csv(ADDITIONAL_CONTEXT_FILE,\n",
    "                                            usecols=['CountryName', 'Population'])\n",
    "country_pop_df['GeoID'] = country_pop_df['CountryName']\n",
    "\n",
    "# US state level population\n",
    "us_states_pop_df = pd.read_csv(ADDITIONAL_US_STATES_CONTEXT,\n",
    "                                              usecols=['NAME', 'POPESTIMATE2019'])\n",
    "# change names so we can simply append the dataframes\n",
    "us_states_pop_df = us_states_pop_df.rename(columns={'POPESTIMATE2019': 'Population'})\n",
    "# GeoID for the states\n",
    "us_states_pop_df['GeoID'] = US_PREFIX + us_states_pop_df['NAME']\n",
    "# Append\n",
    "country_pop_df = country_pop_df.append(us_states_pop_df)\n",
    "# UK population \n",
    "uk_pop_df = pd.read_csv(ADDITIONAL_UK_CONTEXT)\n",
    "# Append\n",
    "country_pop_df = country_pop_df.append(uk_pop_df)\n",
    "# Brazil population\n",
    "brazil_pop_df = pd.read_csv(ADDITIONAL_BRAZIL_CONTEXT)\n",
    "country_pop_df = country_pop_df.append(brazil_pop_df)\n",
    "country_pop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(country_pop_df, on='GeoID', how='left').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-national level data\n",
    "\n",
    "Here we're going to merge data gathered at the sub-national level for USA and the UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US VARIABLES\n",
    "##\n",
    "DATA_PATH = \"../data_sources/\"\n",
    "dius = (pd.read_csv(DATA_PATH + \"HDI_US_States_index.csv\",\n",
    "                    usecols=[\"Country\", \"Region\", \n",
    "                             \"Sub-national HDI\"])\n",
    "        .rename({\"Sub-national HDI\": \"human_development\"}, axis=1))\n",
    "dius['GeoID'] = dius['Country']  + ' / ' + dius['Region']\n",
    "##\n",
    "varsus = (pd.read_csv(DATA_PATH + \"HDI_US_States.csv\")\n",
    "         .drop(columns=[\"ISO_Code\", \"Level\", \"GDLCODE\", \"Expected years schooling\"])\n",
    "         .rename({\"Life expectancy\":\"life_expectancy\",\n",
    "                 \"GNI per capita in thousands of US$ (2011 PPP)\": \"gni_per_capita\",\n",
    "                 \"Mean years schooling\": \"average_years_school\",\n",
    "                 \"Population size in millions\": \"Population\"}, axis=1))\n",
    "varsus['GeoID'] = varsus['Country']  + ' / ' + varsus['Region']\n",
    "state_areas = pd.read_csv(DATA_PATH + \"state-areas.csv\")\n",
    "state_areas['area_km2'] = state_areas[\"area (sq. mi)\"] * 2.58999\n",
    "varsus = (varsus\n",
    "         .merge(state_areas[[\"state\", \"area_km2\"]], left_on=\"Region\", right_on=\"state\")\n",
    "         .drop(columns=[\"state\"]))\n",
    "varsus['Population'] = varsus['Population'] * 1000000\n",
    "varsus['pop_density'] = varsus[\"Population\"] / varsus[\"area_km2\"]\n",
    "varsus = varsus.drop(columns=[\"area_km2\"])\n",
    "urban_pop = pd.read_csv(DATA_PATH + \"urban_pop_us.csv\", usecols=[\"State/Territory\", \"2010\"])\n",
    "urban_pop[\"2010\"] = pd.to_numeric(urban_pop[\"2010\"].map(lambda x: x.rstrip(\"%\")))\n",
    "varsus = (varsus\n",
    "          .merge(urban_pop, left_on=\"Region\", right_on=\"State/Territory\")\n",
    "          .drop(columns=[\"State/Territory\"])\n",
    "          .rename({\"2010\": \"urban_pop\"}, axis=1))\n",
    "total_us_states = varsus.merge(dius.drop(columns=[\"Country\", \"Region\"]), on=\"GeoID\")\n",
    "total_us_states.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK VARIABLES\n",
    "varsuk = (pd.read_csv(DATA_PATH + \"HDI_UK_Regions.csv\")\n",
    "         .drop(columns=[\"ISO_Code\", \"Level\", \"GDLCODE\", \"Expected years schooling\"])\n",
    "         .rename({\"Life expectancy\":\"life_expectancy\",\n",
    "                 \"GNI per capita in thousands of US$ (2011 PPP)\": \"gni_per_capita\",\n",
    "                 \"Mean years schooling\": \"average_years_school\",\n",
    "                 \"Population size in millions\": \"Population\"}, axis=1))\n",
    "varsuk['GeoID'] = varsuk['Country']  + ' / ' + varsuk['Region']\n",
    "# Population\n",
    "pop_uk = pd.read_csv(DATA_PATH + \"pop_data_uk_regions.csv\", usecols=[\"Name\", \"Population\", \"Area\"])\n",
    "pop_uk[\"Population\"] = pd.to_numeric(pop_uk['Population'].map(lambda x: x.replace(\",\", \"\")))\n",
    "pop_uk[\"Area\"] = pop_uk[\"Area\"].map(lambda x: int(x.split(\" \")[0].split(\"k\")[0].replace(\",\", \"\")))\n",
    "scotland = pd.DataFrame({\"Name\": [\"Scotland\"], \"Population\":[5463300], \"Area\":[77933]})\n",
    "wales = pd.DataFrame({\"Name\": [\"Wales\"], \"Population\":[3153000], \"Area\":[20779]})\n",
    "n_ireland = pd.DataFrame({\"Name\": [\"Northern Ireland\"], \"Population\":[1893700], \"Area\":[14130]})\n",
    "pop_uk = pd.concat([pop_uk, scotland, wales, n_ireland])\n",
    "pop_uk[\"GeoID\"] = \"United Kingdom / \" + pop_uk[\"Name\"]\n",
    "varsuk = varsuk.drop(\"Population\", axis=1).merge(pop_uk.drop(columns=['Name']), on=\"GeoID\")\n",
    "# HDI\n",
    "diuk = (pd.read_csv(DATA_PATH + \"HDI_UK_Regions_index.csv\",\n",
    "                   usecols=[\"Country\", \"Region\", \n",
    "                           \"Sub-national HDI\"]\n",
    "                  )\n",
    "        .rename({\"Sub-national HDI\": \"human_development\"}, axis=1))\n",
    "diuk['GeoID'] = diuk['Country']  + ' / ' + diuk['Region']\n",
    "diuk = diuk.drop(columns=[\"Country\", \"Region\"])\n",
    "varsuk = varsuk.merge(diuk, on=\"GeoID\")\n",
    "not_england = ['Northern Ireland', 'Scotland', 'Wales']\n",
    "varsuk.loc[~varsuk.Region.isin(not_england),'Region'] = 'England'\n",
    "varsuk['GeoID'] = varsuk['Country']  + ' / ' + varsuk['Region']\n",
    "varsuk = varsuk.groupby(\"GeoID\").agg({\"life_expectancy\": \"mean\", \n",
    "                            \"gni_per_capita\": \"mean\", \n",
    "                            \"average_years_school\": \"mean\",\n",
    "                            \"Population\": \"sum\",\n",
    "                            \"Area\": \"sum\",\n",
    "                            \"human_development\": \"mean\"}).reset_index()\n",
    "varsuk[\"pop_density\"] = varsuk[\"Population\"] / varsuk[\"Area\"]\n",
    "varsuk[\"urban_pop\"] = None\n",
    "varsuk.loc[varsuk[\"GeoID\"] == \"United Kingdom / England\"] = 83.4\n",
    "varsuk.loc[varsuk[\"GeoID\"] == \"United Kingdom / Northern Ireland\"] = 66.0\n",
    "varsuk.loc[varsuk[\"GeoID\"] == \"United Kingdom / Scotland\"] = 70.0\n",
    "varsuk.loc[varsuk[\"GeoID\"] == \"United Kingdom / Wales\"] = 64.2\n",
    "varsuk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both datasets\n",
    "total_us_states.to_csv(DATA_PATH + \"data_us_states.csv\")\n",
    "varsuk.to_csv(DATA_PATH + \"data_uk_regions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
