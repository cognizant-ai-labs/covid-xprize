## NEAT Prescriptor

This example implements the prescription API using NEAT.
This example will not achieve high performance out-of-the-box.

The goal of this example is to provide an example of how the various
scaffolding can fit together to produce a valid prescriptor.

In particular, the prescriptor uses the input historical IPs file,
the input weights file, along with historical case data to make prescriptions.

This example is unoptimized.
For example, it uses default NEAT parameters,
uses a naiive fitness function,
and provides no method for selecting a tradeoff of evolved solutions.

It also uses a computationally inefficient method of iteratively calling the predictor
that scales quadratically with the number of calls.

### Setup

Assuming you have cloned this repository and your working directory is currently the root of the cloned
repository.

If you have not already done so, install requirements, which include neat-python:
```shell script
pip install -r requirements.txt
```

Ensure that the repository is in your PYTHONPATH:
```shell script
export PYTHONPATH=$PYTHONPATH:$(pwd)
```

### Training Prescriptors

The script `train_prescriptors.py` gives an example of how to train prescriptors.
Here's how it's used. Run this from the root of your cloned repository:
```shell script
python covid_xprize/examples/prescriptors/neat/train_prescriptor.py
```
Periodically, this script saves the current population to `neat-checkpoint-*`.
These saved networks can then be used in prescriptor submissions as explained below. The training script can run for
a long time (potentially indefinitely -- see comments in the script) so you will likely want to kill it with CTRL-C
once it has generated one or more checkpoints for you to test with.

There are MANY ways to improve `train_prescriptors.py`.
See the comments in the file for some example directions for improvement.

The script configures neat-python using the file `config-prescriptor`.
This file contains many options for the underlying algorithm.


### Prescribing with trained Prescriptors

The script `prescribe.py` implements the API required for prescriptor submissions.

As is, it simply loads one of the checkpoints from training, and makes prescriptions with each network 
in that checkpoint.

Say we would like to prescribe using the checkpoint `neat-checkpoint-4`.
Then, open `prescribe.py` and change the line defining `PRESCRIPTORS_FILE` to
```shell script
PRESCRIPTORS_FILE = 'neat-checkpoint-4'
```

Then, prescriptions can be generated by following the API.
For example, say we would like to generate predictions from August 1st to 5th,
and store the results in `predictions/test_prescriptions.csv`. 
Run this from the root of your cloned repository:
```shell script
python covid_xprize/examples/prescriptors/neat/prescribe.py \
    --start_date 2020-08-01 \
    --end_date 2020-08-05 \
    --interventions_past ./covid_xprize/validation/data/2020-09-30_historical_ip.csv \
    --intervention_costs ./covid_xprize/validation/data/uniform_random_costs.csv \
    --output_file prescriptions/test_prescriptions.csv
```

You can validate the prescriptions generated in the above step using the validator module as follows:
```shell script
‚ùØ python covid_xprize/validation/prescriptor_validation.py \
    --start_date 2020-08-01 \
    --end_date 2020-08-05 \
    --interventions_plan ./covid_xprize/validation/data/2020-09-30_historical_ip.csv \
    --submission_file prescriptions/test_prescriptions.csv
``` 
Expected output:
```shell script
2021-01-19 19:35:48 prescriptor_validation INFO     Validating submission file prescriptions/test_prescriptions.csv start date 2020-08-01 end date 2020-08-05 intervention plan ./covid_xprize/validation/data/2020-09-30_historical_ip.csv
2021-01-19 19:35:49 prescriptor_validation INFO     test_prescriptions.csv submission passes validation
2021-01-19 19:35:49 prescriptor_validation INFO     Done!
```

In general, it will be more useful to assemble a set of prescriptors for submission
in a more discerning way, and store and load them in a different way as well.

See the constants defined at the top of `prescribe.py` for other parameters that can be adjusted.


### Evaluating prescription performance

Once prescriptions are generated, different prescriptions for the same sets of geos and days
can be compared using the notebook `prescriptor_robojudge.ipynb` under the root of the repository:
```shell script
jupyter notebook prescriptor_robojudge.ipynb
```

