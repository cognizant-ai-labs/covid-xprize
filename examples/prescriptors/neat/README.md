## NEAT Prescriptor

This example implements the prescription API using NEAT.
This example will not achieve high performance out-of-the-box.

### Setup

These assume the `covid-xprize` repo is located at `<workspace>/covid-xprize`.

If you have not already done so, install requirements, which include neat-python:
```
cd <workspace>/covid-xprize
pip install -r requirements.txt
```

Ensure that `covid-xprize` is in your PYTHONPATH:
```
export PYTHONPATH=$PYTHONPATH:<workspace>/covid-xprize/
```

Setup pretrained LSTM predictor by copying the sample trained weights file to the
location that it loads weights from:
```
cd <workspace>/covid-xprize/examples/predictors/lstm/
mkdir models
cp /tests/fixtures/trained_model_weights_for_tests.h5 models/trained_model_weights.h5
```

### Training Prescriptors

The script `train_prescriptors.py` gives an example of how to train prescriptors.
Here's how it's used:
```
cd <workspace>/covid-xprize/examples/prescriptors/neat/
python train_prescriptors.py
```
Periodically, this script saves the current population to `neat-checkpoint-*`.
These saved networks can then be used in prescriptor submissions.

There are MANY ways to improve `train_prescriptors.py`.
See the comments in the file for some example directions for improvement.

The script configures neat-python using the file `config-prescriptor`.
This file contains many options for the underlying algorithm.



### Prescribing with trained Prescriptors

The script `prescribe.py` implements the API required for prescriptor submissions.

As is, it simply loads one of the checkpoints from training,
and makes prescriptions with each network in that checkpoint.

Say we would like to prescribe using the checkpoint `neat-checkpoint-4`.
Then, open `prescribe.py` and change the line defining `PRESCRIPTORS_FILE` to
```
PRESCRIPTORS_FILE = 'neat-checkpoint-4'
```

Then, prescriptions can be generated by following the API.
For example, say we would like to generate predictions from August 1st to 5th,
and store the results in `test_prescriptions.csv`:
```
cd <workspace>/covid-xprize/examples/prescriptors/neat/
python prescribe.py --start_date 2020-08-01 --end_date 2020-08-05 -ip ../../../validation/data/2020-09-30_historical_ip.csv -c ../../../validation/data/uniform_random_costs.csv -o test_prescriptions.csv
```
Currently, the `-ip` argument is required but ignored, so can be set to anything.
This is because the code currently loads historical IPs from the Oxford data directly.
However, after submission to the real competition, there will not be internet access,
so there may be a gap between the Oxford data and the start_date, in which case the IPs
passed as `-ip` should be used.

In general, it will be more useful to assemble a set of prescriptors for submission
in a more discerning way, and store and load them in a different way as well.


### Evaluating prescription performance

Once prescriptions are generated, different prescriptions for the same sets of geos and days
can be compared using the notebook `<workspace>/covid-xprize/prescriptor_robojudge.ipynb`:
```
cd <workspace>/covid-xprize/
jupyter notebook prescriptor_robojudge.ipynb
```

