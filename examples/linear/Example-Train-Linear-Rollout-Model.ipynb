{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of training a linear model that rolls out predictions day-by-day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Predictor: Linear Rollout Predictor\n",
    "\n",
    "This example contains basic functionality for training and evaluating a linear predictor that rolls out predictions day-by-day.\n",
    "\n",
    "First, a training data set is created from historical case and npi data.\n",
    "\n",
    "Second, a linear model is trained to predict future cases from prior case data along with prior and future npi data.\n",
    "The model is an off-the-shelf sklearn Lasso model, that uses a positive weight constraint to enforce the assumption that increased npis has a negative correlation with future cases.\n",
    "\n",
    "Third, a sample evaluation set is created, and the predictor is applied to this evaluation set to produce prediction results in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main source for the training data\n",
    "DATA_URL = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "# Local file\n",
    "DATA_FILE = 'data/OxCGRT_latest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "urllib.request.urlretrieve(DATA_URL, DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data from local file\n",
    "df = pd.read_csv(DATA_FILE, \n",
    "                 parse_dates=['Date'],\n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, restrict training data to that before a hypothetical predictor submission date\n",
    "HYPOTHETICAL_SUBMISSION_DATE = np.datetime64(\"2020-07-31\")\n",
    "df = df[df.Date <= HYPOTHETICAL_SUBMISSION_DATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RegionID column that combines CountryName and RegionName for easier manipulation of data\n",
    "df['GeoID'] = df['CountryName'] + '__' + df['RegionName'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cases column\n",
    "df['NewCases'] = df.groupby('GeoID').ConfirmedCases.diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns of interest\n",
    "id_cols = ['CountryName',\n",
    "           'RegionName',\n",
    "           'GeoID',\n",
    "           'Date']\n",
    "cases_col = ['NewCases']\n",
    "npi_cols = ['C1_School closing',\n",
    "            'C2_Workplace closing',\n",
    "            'C3_Cancel public events',\n",
    "            'C4_Restrictions on gatherings',\n",
    "            'C5_Close public transport',\n",
    "            'C6_Stay at home requirements',\n",
    "            'C7_Restrictions on internal movement',\n",
    "            'C8_International travel controls',\n",
    "            'H1_Public information campaigns',\n",
    "            'H2_Testing policy',\n",
    "            'H3_Contact tracing']\n",
    "df = df[id_cols + cases_col + npi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing case values by interpolation and setting NaNs to 0\n",
    "df.update(df.groupby('GeoID').NewCases.apply(\n",
    "    lambda group: group.interpolate()).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing NPIs by assuming they are the same as previous day\n",
    "for npi_col in npi_cols:\n",
    "    df.update(df.groupby('GeoID')[npi_col].ffill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of past days to use to make predictions\n",
    "nb_lookback_days = 30\n",
    "\n",
    "# Create training data across all countries for predicting one day ahead\n",
    "X_cols = cases_col + npi_cols\n",
    "y_col = cases_col\n",
    "X_samples = []\n",
    "y_samples = []\n",
    "geo_ids = df.GeoID.unique()\n",
    "for g in geo_ids:\n",
    "    gdf = df[df.GeoID == g]\n",
    "    all_case_data = np.array(gdf[cases_col])\n",
    "    all_npi_data = np.array(gdf[npi_cols])\n",
    "\n",
    "    # Create one sample for each day where we have enough data\n",
    "    # Each sample consists of cases and npis for previous nb_lookback_days\n",
    "    nb_total_days = len(gdf)\n",
    "    for d in range(nb_lookback_days, nb_total_days - 1):\n",
    "        X_cases = all_case_data[d-nb_lookback_days:d]\n",
    "\n",
    "        # Take negative of npis to support positive\n",
    "        # weight constraint in Lasso.\n",
    "        X_npis = -all_npi_data[d - nb_lookback_days:d]\n",
    "\n",
    "        # Flatten all input data so it fits Lasso input format.\n",
    "        X_sample = np.concatenate([X_cases.flatten(),\n",
    "                                   X_npis.flatten()])\n",
    "        y_sample = all_case_data[d + 1]\n",
    "        X_samples.append(X_sample)\n",
    "        y_samples.append(y_sample)\n",
    "\n",
    "X_samples = np.array(X_samples)\n",
    "y_samples = np.array(y_samples).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful function to compute mae\n",
    "def mae(pred, true):\n",
    "    return np.mean(np.abs(pred - true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_samples,\n",
    "                                                    y_samples,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Lasso model.\n",
    "# Set positive=True to enforce assumption that cases are positively correlated\n",
    "# with future cases and npis are negatively correlated.\n",
    "model = Lasso(alpha=0.1,\n",
    "              precompute=True,\n",
    "              max_iter=10000,\n",
    "              positive=True,\n",
    "              selection='random')\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "train_preds = model.predict(X_train)\n",
    "train_preds = np.maximum(train_preds, 0) # Don't predict negative cases\n",
    "print('Train MAE:', mae(train_preds, y_train))\n",
    "\n",
    "test_preds = model.predict(X_test)\n",
    "test_preds = np.maximum(test_preds, 0) # Don't predict negative cases\n",
    "print('Test MAE:', mae(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the learned feature coefficients for the model\n",
    "# to see what features it's paying attention to.\n",
    "\n",
    "# Give names to the features\n",
    "x_col_names = []\n",
    "for d in range(-nb_lookback_days, 0):\n",
    "    x_col_names.append('Day ' + str(d) + ' ' + cases_col[0])\n",
    "for d in range(-nb_lookback_days, 1):\n",
    "    for col_name in npi_cols:\n",
    "        x_col_names.append('Day ' + str(d) + ' ' + col_name)\n",
    "\n",
    "# View non-zero coefficients\n",
    "for (col, coeff) in zip(x_col_names, list(model.coef_)):\n",
    "    if coeff != 0.:\n",
    "        print(col, coeff)\n",
    "print('Intercept', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to file\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that the predictor has been trained and saved, this section contains the functionality for evaluating it on sample evaluation data.\n",
    "\n",
    "First, a sample evaluation data set is created of the form that is given to the predictor.\n",
    "\n",
    "Second, the predictor is evaluated on this data set, and a resulting predictions file is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hypothetical evaluation data\n",
    "nb_eval_days = 31\n",
    "test_df = pd.read_csv(DATA_URL, \n",
    "                      parse_dates=['Date'],\n",
    "                      encoding=\"ISO-8859-1\",\n",
    "                      error_bad_lines=False)\n",
    "\n",
    "# Pull out relevant evaluation days\n",
    "test_df = test_df[(test_df.Date > HYPOTHETICAL_SUBMISSION_DATE) & \\\n",
    "                  (test_df.Date <= HYPOTHETICAL_SUBMISSION_DATE + nb_eval_days)]\n",
    "\n",
    "# Only include columns we would see during evaluation\n",
    "test_df = test_df[['CountryName', 'RegionName', 'Date'] + npi_cols]\n",
    "\n",
    "# Fill any missing NPIs by assuming they are the same as previous day\n",
    "for npi_col in npi_cols:\n",
    "    test_df.update(test_df.groupby(['CountryName', 'RegionName'])[npi_col].ffill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df is now in the form of input to a predictor during evaluation\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply predictor to the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(start_date: str, end_date: str, path_to_ips_file: str):\n",
    "    \"\"\"\n",
    "    Generates a file with daily new cases predictions for the given countries, regions and npis, between\n",
    "    start_date and end_date, included.\n",
    "    :param start_date: day from which to start making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param end_date: day on which to stop making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param path_to_ips_file: path to a csv file containing the intervention plans between start_date and end_date\n",
    "    :return: Nothing. Saves a csv file called 'start_date_end_date.csv'\n",
    "    with columns \"CountryName,RegionName,Date,PredictedDailyNewCases\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add RegionID column that combines CountryName and RegionName for easier manipulation of data\\n\",\n",
    "    test_df['GeoID'] = test_df['CountryName'] + '__' + test_df['RegionName'].astype(str)\n",
    "\n",
    "    # Copy the test data frame\n",
    "    pred_df = test_df[id_cols].copy()\n",
    "    # Keep only the requested prediction period.\n",
    "    # Note: this period *might* be in the future, and pred_df doesn't necessarily contain the requested rows\n",
    "    pred_df = pred_df[(pred_df.Date >= start_date) & (pred_df.Date <= end_date)]\n",
    "\n",
    "    # Load historical data to use in making predictions in the same way \n",
    "    # df is loaded above to make training data (just copy the reference here for simplicity)\n",
    "    hist_df = df\n",
    "    \n",
    "    # Load model\n",
    "    with open('model.pkl', 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "        \n",
    "    # Make predictions for each country,region pair\n",
    "    geo_pred_dfs = []\n",
    "    for g in test_df.GeoID.unique():\n",
    "        print('\\nPredicting for', g)\n",
    "\n",
    "        # Pull out all relevant data for country c\n",
    "        hist_gdf = hist_df[hist_df.GeoID == g]\n",
    "        test_gdf = test_df[test_df.GeoID == g]\n",
    "        past_cases = np.array(hist_gdf[cases_col])\n",
    "        past_npis = np.array(hist_gdf[npi_cols])\n",
    "        future_npis = np.array(test_gdf[npi_cols])\n",
    "\n",
    "        # Make prediction for each day\n",
    "        geo_preds = []\n",
    "        nb_days_to_predict = len(future_npis)\n",
    "        for days_ahead in range(nb_days_to_predict):\n",
    "\n",
    "            # Prepare data\n",
    "            X_cases = past_cases[-nb_lookback_days:]\n",
    "            X_npis = past_npis[-nb_lookback_days:]\n",
    "            X = np.concatenate([X_cases.flatten(),\n",
    "                                X_npis.flatten()])\n",
    "\n",
    "            # Make the prediction (reshape so that sklearn is happy)\n",
    "            pred = model.predict(X.reshape(1, -1))[0]\n",
    "            pred = max(0, pred) # Do not allow predicting negative cases\n",
    "            geo_preds.append(pred)\n",
    "            print(pred)\n",
    "            \n",
    "            # Append the prediction and npi's for next day\n",
    "            # in order to rollout predictions for further days.\n",
    "            past_cases = np.append(past_cases, pred)\n",
    "            past_npis = np.append(past_npis, future_npis[days_ahead:days_ahead+1], axis=0)\n",
    "\n",
    "        # Create geo_pred_df with pred column\n",
    "        geo_pred_df = test_gdf[id_cols].copy()\n",
    "        geo_pred_df['PredictedDailyNewCases'] = geo_preds\n",
    "        geo_pred_dfs.append(geo_pred_df)\n",
    "\n",
    "    # Combine all predictions into a single dataframe\n",
    "    pred_df = pd.concat(geo_pred_dfs)\n",
    "    \n",
    "    # Drop GeoID column to match expected output format\n",
    "    pred_df = pred_df.drop(columns=['GeoID'])\n",
    "    pred_df\n",
    "    \n",
    "    # Write predictions to csv\n",
    "    # Save to expected file name\n",
    "    output_file_name = start_date + \"_\" + end_date + \".csv\"\n",
    "    pred_df.to_csv(output_file_name, index=None)\n",
    "    print(f\"Predictions saved to {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2020-08-01\"\n",
    "end_date = \"2020-08-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predict(start_date, end_date, path_to_ips_file=\"../validation/data/2020-08-01_2020-08-31_ip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that predictions are written correctly\n",
    "!head 2020-08-01_2020-08-31.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "from validation.validation import validate_submission\n",
    "\n",
    "errors = validate_submission(start_date, end_date, \"2020-08-01_2020-08-31.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "This is how the predictor is going to be called during the competition.  \n",
    "!!! PLEASE DO NOT CHANGE THE API !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py -s 2020-08-01 -e 2020-08-04 -ip ../../validation/data/2020-08-01_2020-08-04_ip.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "from validation.validation import validate_submission\n",
    "\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-08-04\", \"2020-08-01_2020-08-04.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py -s 2020-08-01 -e 2020-08-31 -ip ../../validation/data/2020-08-01_2020-08-31_ip.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-08-31\", \"2020-08-01_2020-08-31.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!python predict.py -s 2020-08-01 -e 2020-09-30 -ip ../../validation/data/2020-08-01_2020-09-30_ip.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-09-30\", \"2020-08-01_2020-09-30.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
