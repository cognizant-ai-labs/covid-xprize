{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of training a linear model that rolls out predictions day-by-day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Predictor: Linear Rollout Predictor\n",
    "\n",
    "This example contains basic functionality for training and evaluating a linear predictor that rolls out predictions day-by-day.\n",
    "\n",
    "First, a training data set is created from historical case and npi data.\n",
    "\n",
    "Second, a linear model is trained to predict future cases from prior case data along with prior and future npi data.\n",
    "The model is an off-the-shelf sklearn Lasso model, that uses a positive weight constraint to enforce the assumption that increased npis has a negative correlation with future cases.\n",
    "\n",
    "Third, a sample evaluation set is created, and the predictor is applied to this evaluation set to produce prediction results in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main source for the training data\n",
    "DATA_URL = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "# Local file\n",
    "DATA_FILE = 'data/OxCGRT_latest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "urllib.request.urlretrieve(DATA_URL, DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data from local file\n",
    "df = pd.read_csv(DATA_FILE, \n",
    "                 parse_dates=['Date'],\n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, restrict training data to that before a hypothetical predictor submission date\n",
    "HYPOTHETICAL_SUBMISSION_DATE = np.datetime64(\"2020-07-31\")\n",
    "df = df[df.Date <= HYPOTHETICAL_SUBMISSION_DATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RegionID column that combines CountryName and RegionName for easier manipulation of data\n",
    "df['GeoID'] = df['CountryName'] + '__' + df['RegionName'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cases column\n",
    "df['NewCases'] = df.groupby('GeoID').ConfirmedCases.diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns of interest\n",
    "id_cols = ['CountryName',\n",
    "           'RegionName',\n",
    "           'GeoID',\n",
    "           'Date']\n",
    "cases_col = ['NewCases']\n",
    "npi_cols = ['C1_School closing',\n",
    "            'C2_Workplace closing',\n",
    "            'C3_Cancel public events',\n",
    "            'C4_Restrictions on gatherings',\n",
    "            'C5_Close public transport',\n",
    "            'C6_Stay at home requirements',\n",
    "            'C7_Restrictions on internal movement',\n",
    "            'C8_International travel controls',\n",
    "            'H1_Public information campaigns',\n",
    "            'H2_Testing policy',\n",
    "            'H3_Contact tracing']\n",
    "df = df[id_cols + cases_col + npi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing case values by interpolation and setting NaNs to 0\n",
    "df.update(df.groupby('GeoID').NewCases.apply(\n",
    "    lambda group: group.interpolate()).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing NPIs by assuming they are the same as previous day\n",
    "for npi_col in npi_cols:\n",
    "    df.update(df.groupby('GeoID')[npi_col].ffill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of past days to use to make predictions\n",
    "nb_lookback_days = 30\n",
    "\n",
    "# Create training data across all countries for predicting one day ahead\n",
    "X_cols = cases_col + npi_cols\n",
    "y_col = cases_col\n",
    "X_samples = []\n",
    "y_samples = []\n",
    "geo_ids = df.GeoID.unique()\n",
    "for g in geo_ids:\n",
    "    gdf = df[df.GeoID == g]\n",
    "    all_case_data = np.array(gdf[cases_col])\n",
    "    all_npi_data = np.array(gdf[npi_cols])\n",
    "\n",
    "    # Create one sample for each day where we have enough data\n",
    "    # Each sample consists of cases and npis for previous nb_lookback_days\n",
    "    nb_total_days = len(gdf)\n",
    "    for d in range(nb_lookback_days, nb_total_days - 1):\n",
    "        X_cases = all_case_data[d-nb_lookback_days:d]\n",
    "\n",
    "        # Take negative of npis to support positive\n",
    "        # weight constraint in Lasso.\n",
    "        X_npis = -all_npi_data[d - nb_lookback_days:d]\n",
    "\n",
    "        # Flatten all input data so it fits Lasso input format.\n",
    "        X_sample = np.concatenate([X_cases.flatten(),\n",
    "                                   X_npis.flatten()])\n",
    "        y_sample = all_case_data[d + 1]\n",
    "        X_samples.append(X_sample)\n",
    "        y_samples.append(y_sample)\n",
    "\n",
    "X_samples = np.array(X_samples)\n",
    "y_samples = np.array(y_samples).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful function to compute mae\n",
    "def mae(pred, true):\n",
    "    return np.mean(np.abs(pred - true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_samples,\n",
    "                                                    y_samples,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Lasso model.\n",
    "# Set positive=True to enforce assumption that cases are positively correlated\n",
    "# with future cases and npis are negatively correlated.\n",
    "model = Lasso(alpha=0.1,\n",
    "              precompute=True,\n",
    "              max_iter=10000,\n",
    "              positive=True,\n",
    "              selection='random')\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "train_preds = model.predict(X_train)\n",
    "train_preds = np.maximum(train_preds, 0) # Don't predict negative cases\n",
    "print('Train MAE:', mae(train_preds, y_train))\n",
    "\n",
    "test_preds = model.predict(X_test)\n",
    "test_preds = np.maximum(test_preds, 0) # Don't predict negative cases\n",
    "print('Test MAE:', mae(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the learned feature coefficients for the model\n",
    "# to see what features it's paying attention to.\n",
    "\n",
    "# Give names to the features\n",
    "x_col_names = []\n",
    "for d in range(-nb_lookback_days, 0):\n",
    "    x_col_names.append('Day ' + str(d) + ' ' + cases_col[0])\n",
    "for d in range(-nb_lookback_days, 1):\n",
    "    for col_name in npi_cols:\n",
    "        x_col_names.append('Day ' + str(d) + ' ' + col_name)\n",
    "\n",
    "# View non-zero coefficients\n",
    "for (col, coeff) in zip(x_col_names, list(model.coef_)):\n",
    "    if coeff != 0.:\n",
    "        print(col, coeff)\n",
    "print('Intercept', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to file\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "with open('models/model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that the predictor has been trained and saved, this section contains the functionality for evaluating it on sample evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module to get the latest changes\n",
    "import predict\n",
    "from importlib import reload\n",
    "reload(predict)\n",
    "from predict import predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds_df = predict_df(\"2020-08-01\", \"2020-08-31\", path_to_ips_file=\"../../validation/data/2020-08-01_2020-08-31_ip.csv\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the predictions\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "This is how the predictor is going to be called during the competition.  \n",
    "!!! PLEASE DO NOT CHANGE THE API !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py -s 2020-08-01 -e 2020-08-04 -ip ../../validation/data/2020-09-30_historical_ip.csv -o predictions/2020-08-01_2020-08-04.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "from validation.validation import validate_submission\n",
    "\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-08-04\", \"predictions/2020-08-01_2020-08-04.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py -s 2020-08-01 -e 2020-08-31 -ip ../../validation/data/2020-08-01_2020-08-31_ip.csv -o predictions/2020-08-01_2020-08-31.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-08-31\", \"predictions/2020-08-01_2020-08-31.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!python predict.py -s 2020-08-01 -e 2020-09-30 -ip ../../validation/data/2020-08-01_2020-09-30_ip.csv -o predictions/2020-08-01_2020-09-30.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-09-30\", \"predictions/2020-08-01_2020-09-30.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
